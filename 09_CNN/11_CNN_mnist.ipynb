{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST dataset contains grayscale images of handwritten digits (0-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the MNIST dataset\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2a3e51acd90>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,2))\n",
    "plt.imshow(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]]], dtype=uint8)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, ..., 5, 6, 8], dtype=uint8)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaling pixel values to the range [0, 1]\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the input data to fit the expected input shape of the CNN\n",
    "x_train = x_train.reshape((-1, 28, 28, 1))\n",
    "x_test = x_test.reshape((-1, 28, 28, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nitha\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(\n"
     ]
    }
   ],
   "source": [
    "# Define the CNN architecture\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sparse Categorical Crossentropy: This loss function is used when the targets are integers. It computes the crossentropy loss between the integer labels and the probability predictions.\n",
    "\n",
    "Categorical Crossentropy: This loss function is used when the targets are one-hot encoded. It computes the crossentropy loss between the labels and predictions.\n",
    "\n",
    "the value of y_train is [6, 9, 9, ..., 9, 1, 1].  So Sparse Categorical Crossentropy is used as loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 11ms/step - accuracy: 0.8628 - loss: 0.4447 - val_accuracy: 0.9783 - val_loss: 0.0668\n",
      "Epoch 2/5\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 11ms/step - accuracy: 0.9844 - loss: 0.0521 - val_accuracy: 0.9880 - val_loss: 0.0381\n",
      "Epoch 3/5\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - accuracy: 0.9890 - loss: 0.0342 - val_accuracy: 0.9891 - val_loss: 0.0330\n",
      "Epoch 4/5\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 11ms/step - accuracy: 0.9919 - loss: 0.0260 - val_accuracy: 0.9904 - val_loss: 0.0317\n",
      "Epoch 5/5\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 10ms/step - accuracy: 0.9941 - loss: 0.0190 - val_accuracy: 0.9904 - val_loss: 0.0272\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2a3e9f1ee50>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(x_train, y_train, epochs=5, batch_size=64, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9868 - loss: 0.0347\n",
      "Test accuracy: 0.9904000163078308\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print(\"Test accuracy:\", test_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
